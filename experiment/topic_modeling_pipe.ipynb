{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Machine Learning Pipeline using Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create a sklearn pipeline in order to make life a lot easier when scoring new data i.e. articles which have to undergo the same preprocessing and modeling strategy as the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nutzer\\.conda\\envs\\NLP\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path variables\n",
    "DATAPATH = Path(Path.cwd().parents[0] / 'data')\n",
    "MODELPATH = Path(Path.cwd().parents[0] / 'model')\n",
    "\n",
    "DATAPATH.mkdir(parents=True, exist_ok=True)\n",
    "MODELPATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_topic(pipeline_obj:Pipeline, article:pd.DataFrame, article_num:int):\n",
    "    \"\"\"Generate article topics for given article and given i.e. trained pipeline\n",
    "    \"\"\"\n",
    "    topic_num = np.argmax(pipeline_obj.transform([article[article_num]]))\n",
    "    return topic_num \n",
    "\n",
    "def get_topic_terms(pipe_obj:Pipeline, topic_num=0, top_n=3):\n",
    "    \"\"\" get the index position of the top 3 terms in a topic.\n",
    "    input the fitted laten dirichtlet object.\n",
    "    input the fitted count-vectorizer object.\n",
    "    input the topic number.\n",
    "    input the top-n words belonging to each topic.\n",
    "    output the top_n words for topic_num. \"\"\"\n",
    "    return [pipe_obj['cv'].get_feature_names_out()[ind] for ind in pipe_obj['lda'].components_[topic_num].argsort()[-top_n:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv(DATAPATH / \"npr.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Tokenizer Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write custom tokenizer class to be passed to CountVectorizer instance \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        lemma_tokens = [self.wnl.lemmatize(t) for t in word_tokenize(articles)] # lemmatize words\n",
    "        lemma_tokens_alpha = [t for t in lemma_tokens if t.isalpha()] # ensure words are alphabetic\n",
    "        lemma_tokens_alpha_long = [t for t in lemma_tokens_alpha if len(t)>=3] # ensure minimum word lengths\n",
    "        return lemma_tokens_alpha_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a few stopwords\n",
    "my_additional_stop_words = ['ha', 'le', 'u', 'wa']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Preprocessing and Modeling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init pipeline object\n",
    "pipe = Pipeline([('cv', CountVectorizer(lowercase=True,\n",
    "                                        #stop_words='english',\n",
    "                                        stop_words=stop_words, # use my extended list of stop words\n",
    "                                        tokenizer=LemmaTokenizer(),\n",
    "                                        ngram_range=(1,1),\n",
    "                                        max_df=.9, \n",
    "                                        min_df=.05,\n",
    "                                        max_features=1000 \n",
    "                                        )), \n",
    "                ('lda', LatentDirichletAllocation(n_components=10,\n",
    "                                                  random_state=123,\n",
    "                                                  n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split\n",
    "We use a train test split to train the model on the training data and use the test data later on just for scoring kind of \"new\" data; the test data is of course not used by any means to test the model since it´s an unsupervised case with the ground truth being unknown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8854</th>\n",
       "      <td>Welcome to the third installment of Read, Watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3298</th>\n",
       "      <td>Milwaukee has the nation’s   publicly funded v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>Donald Trump has released the names of his eco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>South Korea and the U. S. confirm North Korea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>A   international convoy carrying desperately ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Article\n",
       "8854  Welcome to the third installment of Read, Watc...\n",
       "3298  Milwaukee has the nation’s   publicly funded v...\n",
       "8716  Donald Trump has released the names of his eco...\n",
       "2246  South Korea and the U. S. confirm North Korea ...\n",
       "7008  A   international convoy carrying desperately ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save test data as csv file for later scoring\n",
    "X_test.to_csv(DATAPATH /'npr_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the pipeline\n",
    "pipe.fit(X_train.Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained pipeline model\n",
    "pickle.dump(pipe, open(MODELPATH / 'pipe_model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the functionality of the fitted pipeline and all its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.08878385e-01, 2.82548644e-04, 2.82556483e-04, 2.82553258e-04,\n",
       "        2.82553023e-04, 2.82555312e-04, 2.42628428e-02, 2.82544473e-04,\n",
       "        5.09450070e-02, 5.14218455e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the whole pipeline ie apply it to the first article...we expect an array with topic probabilities\n",
    "pipe.transform([X_train['Article'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x978 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 197 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the count vectorizer part of the pipeline ie apply it to the first article\n",
    "dtm = pipe['cv'].transform([X_train['Article'][0]])\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nutzer\\.conda\\envs\\NLP\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['writer',\n",
       " 'writes',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'year',\n",
       " 'yes',\n",
       " 'york',\n",
       " 'young']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the terms\n",
    "pipe['cv'].get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.08878385e-01, 2.82548644e-04, 2.82556483e-04, 2.82553258e-04,\n",
       "        2.82553023e-04, 2.82555312e-04, 2.42628428e-02, 2.82544473e-04,\n",
       "        5.09450070e-02, 5.14218455e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the lda part of the pipeline ie apply it to the first article\n",
    "pipe['lda'].transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.38036783e+01, 1.39500262e+02, 2.38628861e+01, ...,\n",
       "        1.23897442e+02, 5.12709381e+02, 8.24055752e+01],\n",
       "       [1.63442873e+02, 3.24633594e+02, 1.86475641e+02, ...,\n",
       "        3.88589007e+01, 8.16695987e+01, 1.39606030e+02],\n",
       "       [3.95461911e+01, 4.24780261e+02, 2.60639167e+02, ...,\n",
       "        4.46330601e+01, 2.89979667e+02, 1.58123414e+02],\n",
       "       ...,\n",
       "       [5.29379506e+01, 7.31809994e+01, 1.33144608e+02, ...,\n",
       "        1.85740750e+01, 1.56915206e+02, 1.00041419e-01],\n",
       "       [1.53347316e+02, 5.58021438e+02, 1.74043972e+01, ...,\n",
       "        4.40423786e+02, 4.63731024e+02, 9.29886926e+02],\n",
       "       [1.01626771e+02, 1.21435535e+02, 2.22278325e+02, ...,\n",
       "        5.01889224e+00, 7.35283428e+01, 2.39248508e+00]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look at the fit result i.e. the topics\n",
    "pipe['lda'].components_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score New Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pipeline model\n",
    "pipe = pickle.load(open(MODELPATH / 'pipe_model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hamilton, the Broadway musical about ”the scra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tennessee Ernie Ford was fed up with the trapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When Yomi Wrong was born in 1972, doctors told...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As laptops become smaller and more ubiquitous,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republican presidential nominee Donald Trump i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  Hamilton, the Broadway musical about ”the scra...\n",
       "1  Tennessee Ernie Ford was fed up with the trapp...\n",
       "2  When Yomi Wrong was born in 1972, doctors told...\n",
       "3  As laptops become smaller and more ubiquitous,...\n",
       "4  Republican presidential nominee Donald Trump i..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import txt data as dataframe\n",
    "X_test = pd.read_csv(DATAPATH / 'npr_test.csv')\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3958, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many articles we have\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main topic for article #100 is topic #6\n",
      "The top 5 terms for topic #6 are: \n",
      " ['country', 'said', 'family', 'say', 'woman']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get article topic for single article\n",
    "article_num = 100\n",
    "top_n = 5\n",
    "topic_num = get_article_topic(pipe, X_test.Article, article_num)\n",
    "top_terms = get_topic_terms(pipe, topic_num, top_n)\n",
    "print('The main topic for article #{} is topic #{}'.format(article_num, topic_num))\n",
    "print('The top {} terms for topic #{} are: \\n {}'.format(top_n, topic_num, top_terms))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15aef1ab2b9b673ef1593735a79ec64b54834f781f07d581b1bf012a72b966cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
