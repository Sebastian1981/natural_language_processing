{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Topics for Numerous Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Azure Machine Learning Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.41.0\n",
      "Workspace sd_ml loaded\n",
      "Compute Resources:\n",
      "\t myCluster : AmlCompute\n",
      "\t myComputer : ComputeInstance\n"
     ]
    }
   ],
   "source": [
    "# Import the azureml-core package and checking the version of the SDK that is installed\n",
    "import azureml.core\n",
    "print(\"Ready to use Azure ML\", azureml.core.VERSION)\n",
    "\n",
    "# connect to workspace\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace', ws.name, \"loaded\")\n",
    "\n",
    "# view azure compute ressources\n",
    "print(\"Compute Resources:\")\n",
    "for compute_name in ws.compute_targets:\n",
    "    compute = ws.compute_targets[compute_name]\n",
    "    print(\"\\t\", compute.name, ':', compute.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path variables\n",
    "DATAPATH = Path('data')\n",
    "MODELPATH = Path('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv(DATAPATH / \"npr.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the Washington of 2016, even when the policy can be bipartisan, the politics cannot. And in that sense, this year shows little sign of ending on Dec. 31. When President Obama moved to sanction Russia over its alleged interference in the U. S. election just concluded, some Republicans who had long called for similar or more severe measures could scarcely bring themselves to approve. House Speaker Paul Ryan called the Obama measures ”appropriate” but also ”overdue” and ”a prime example of this administration’s ineffective foreign policy that has left America weaker in the eyes of the world.” Other GOP leaders sounded much the same theme. ”[We have] been urging President Obama for years to take strong action to deter Russia’s worldwide aggression, including its   operations,” wrote Rep. Devin Nunes,  . chairman of the House Intelligence Committee. ”Now with just a few weeks left in office, the president has suddenly decided that some stronger measures are indeed warranted.” Appearing o'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get an overview of a specific article\n",
    "df.Article[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11992"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of articles\n",
    "len(df.Article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write custom tokenizer class to be passed to CountVectorizer instance \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        lemma_tokens = [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "        lemma_tokens_alpha = [t for t in lemma_tokens if t.isalpha()]\n",
    "        return lemma_tokens_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the countvectorizer object\n",
    "cv = CountVectorizer(lowercase=True,\n",
    "                     stop_words='english',\n",
    "                     tokenizer=LemmaTokenizer(),\n",
    "                     ngram_range=(1,1),\n",
    "                     max_df=.9, \n",
    "                     min_df=.05,\n",
    "                     max_features=1000 \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fit and transform the cv object to create a document term matrix\n",
    "dtm = cv.fit_transform(df.Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape: (11992, 979)\n"
     ]
    }
   ],
   "source": [
    "# check type and shape of dtm\n",
    "print('type:', type(dtm))\n",
    "print('shape:', dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check sparsity - high sparsity expected\n",
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['writer',\n",
       " 'writes',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'year',\n",
       " 'yes',\n",
       " 'york',\n",
       " 'young']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the terms\n",
    "cv.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the LDA object\n",
    "LDA = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_jobs=-1, random_state=123)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the LDA object\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.43195650e+02, 2.34134679e+02, 6.01013378e+02, ...,\n",
       "        6.82072223e+00, 5.01368642e+02, 1.06639117e-01],\n",
       "       [1.14387199e+02, 1.98434510e+02, 1.47439641e+02, ...,\n",
       "        5.62634844e+01, 1.61976624e+02, 1.43655510e+03],\n",
       "       [2.98953324e+01, 3.09574025e+02, 4.07509224e+02, ...,\n",
       "        1.75718095e+01, 7.06604170e+01, 1.03160090e+02],\n",
       "       ...,\n",
       "       [2.77909753e+02, 6.15394106e+02, 2.30462459e+02, ...,\n",
       "        6.10713898e+01, 1.86539814e+02, 1.35200342e+02],\n",
       "       [1.10928781e+02, 1.37682785e+02, 6.74706397e+01, ...,\n",
       "        8.77417950e+01, 6.71249813e+02, 9.21314930e+00],\n",
       "       [3.63136927e+01, 7.67916003e+01, 2.43411914e+01, ...,\n",
       "        6.45430157e+01, 3.26636876e+02, 3.04070900e+02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look at the fit result i.e. the topics\n",
    "LDA.components_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:  <class 'numpy.ndarray'>\n",
      "shape:  (10, 979)\n"
     ]
    }
   ],
   "source": [
    "# get type and shape of topics\n",
    "print('type: ', type(LDA.components_))\n",
    "print('shape: ', LDA.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_topic(lda_obj, article_num):\n",
    "    \"\"\" input LDA object and the number of the article to be scored.\n",
    "    Return the topic number for that article\"\"\"\n",
    "    topic_num = np.argmax(lda_obj.transform(dtm[article_num]))\n",
    "    return topic_num \n",
    "\n",
    "def get_topic_terms(lda_obj, cv_obj, topic_num=0, top_n=3):\n",
    "    \"\"\" get the index position of the top 3 terms in a topic.\n",
    "    input the fitted laten dirichtlet object.\n",
    "    input the fitted count-vectorizer object.\n",
    "    input the topic number.\n",
    "    input the top-n words belonging to each topic.\n",
    "    output the top_n words for topic_num. \"\"\"\n",
    "      \n",
    "    return [cv_obj.get_feature_names_out()[ind] for ind in lda_obj.components_[topic_num].argsort()[-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic for article number 0 is 8\n",
      "The top 5 terms in article number 0 are: \n",
      " ['clinton', 'wa', 'president', 'said', 'trump']\n"
     ]
    }
   ],
   "source": [
    "article_num = 0 # select article number\n",
    "top_n = 5 # select top n terms\n",
    "topic_num = get_article_topic(LDA, article_num)\n",
    "print('The topic for article number {} is {}'.format(article_num, topic_num))\n",
    "\n",
    "top_terms = get_topic_terms(LDA, cv, topic_num, top_n)\n",
    "print('The top {} terms in article number {} are: \\n {}'.format(top_n, article_num, top_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the Washington of 2016, even when the policy can be bipartisan, the politics cannot. And in that sense, this year shows little sign of ending on Dec. 31. When President Obama moved to sanction Russia over its alleged interference in the U. S. election just concluded, some Republicans who had long called for similar or more severe measures could scarcely bring themselves to approve. House Speaker Paul Ryan called the Obama measures ”appropriate” but also ”overdue” and ”a prime example of this '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Article[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article #0:\n",
      "In the Washington of 2016, even when the policy can be bipartisan, the politics cannot. And in that sense, this year shows little sign of ending on Dec. 31. When President Obama moved to sanction Russ\n",
      "Article number #0 contains topic #8\n",
      "The top 10 terms for topic #8 are: \n",
      " ['republican', 'obama', 'house', 'campaign', 'ha', 'clinton', 'wa', 'president', 'said', 'trump']\n",
      "\n",
      "\n",
      "Article #1:\n",
      "  Donald Trump has used Twitter  —   his preferred means of communication  —   to weigh in on a swath of foreign policy issues over the past few weeks. His comments give a glimpse into how his incomin\n",
      "Article number #1 contains topic #8\n",
      "The top 10 terms for topic #8 are: \n",
      " ['republican', 'obama', 'house', 'campaign', 'ha', 'clinton', 'wa', 'president', 'said', 'trump']\n",
      "\n",
      "\n",
      "Article #2:\n",
      "  Donald Trump is unabashedly praising Russian President Vladimir Putin, a day after outgoing President Obama issued tough sanctions against the country in response to alleged cyberattacks intended to\n",
      "Article number #2 contains topic #8\n",
      "The top 10 terms for topic #8 are: \n",
      " ['republican', 'obama', 'house', 'campaign', 'ha', 'clinton', 'wa', 'president', 'said', 'trump']\n",
      "\n",
      "\n",
      "Article #3:\n",
      "Updated at 2:50 p. m. ET, Russian President Vladimir Putin says Russia won’t be expelling U. S. diplomats in a    response to U. S. sanctions, as his foreign minister had suggested earlier Friday. Ins\n",
      "Article number #3 contains topic #8\n",
      "The top 10 terms for topic #8 are: \n",
      " ['republican', 'obama', 'house', 'campaign', 'ha', 'clinton', 'wa', 'president', 'said', 'trump']\n",
      "\n",
      "\n",
      "Article #4:\n",
      "From photography, illustration and video, to data visualizations and immersive experiences, visuals are an important part of our storytelling at NPR. Interwoven with the written and the spoken word, i\n",
      "Article number #4 contains topic #0\n",
      "The top 10 terms for topic #0 are: \n",
      " ['new', 'said', 'federal', 'government', 'court', 'law', 'state', 'company', 'say', 'ha']\n",
      "\n",
      "\n",
      "Article #5:\n",
      "I did not want to join yoga class. I hated those   beatific instructors. I worried that the people in the class could fold up like origami and I’d fold up like a bread stick. I understood the need for\n",
      "Article number #5 contains topic #3\n",
      "The top 10 terms for topic #3 are: \n",
      " ['really', 'thing', 'know', 'think', 'just', 'like', 'people', 'say', 't', 'wa']\n",
      "\n",
      "\n",
      "Article #6:\n",
      "With a   who has publicly supported the debunked claim that vaccines cause autism, suggested that climate change is a hoax dreamed up by the Chinese, and appointed to his Cabinet a retired neurosurgeo\n",
      "Article number #6 contains topic #7\n",
      "The top 10 terms for topic #7 are: \n",
      " ['wa', 'disease', 'water', 'year', 'food', 'ha', 'study', 't', 'people', 'say']\n",
      "\n",
      "\n",
      "Article #7:\n",
      "I was standing by the airport exit, debating whether to get a snack, when a young man with a round face approached me. I focused hard to decipher his words. In a thick accent, he asked me to help him \n",
      "Article number #7 contains topic #3\n",
      "The top 10 terms for topic #3 are: \n",
      " ['really', 'thing', 'know', 'think', 'just', 'like', 'people', 'say', 't', 'wa']\n",
      "\n",
      "\n",
      "Article #8:\n",
      "If movies were trying to be more realistic, perhaps the way to summon Batman shouldn’t have been the    —   it should have been the bat squeak. New research from the Bat Lab for   at Tel Aviv Universi\n",
      "Article number #8 contains topic #7\n",
      "The top 10 terms for topic #7 are: \n",
      " ['wa', 'disease', 'water', 'year', 'food', 'ha', 'study', 't', 'people', 'say']\n",
      "\n",
      "\n",
      "Article #9:\n",
      "Eighteen years ago, on New Year’s Eve, David Fisher visited an old farm in western Massachusetts, near the small town of Conway. No one was farming there at the time, and that’s what had drawn Fisher \n",
      "Article number #9 contains topic #3\n",
      "The top 10 terms for topic #3 are: \n",
      " ['really', 'thing', 'know', 'think', 'just', 'like', 'people', 'say', 't', 'wa']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the top terms for the first 10 articles\n",
    "top_n = 10 # select top n terms\n",
    "for article_num in range(0,10):\n",
    "    print('Article #{}:'.format(article_num))\n",
    "    print(df.Article[article_num][:200])\n",
    "    topic_num = get_article_topic(LDA, article_num)\n",
    "    top_terms = get_topic_terms(LDA, cv, topic_num, top_n)\n",
    "    print('Article number #{} contains topic #{}'.format(article_num, topic_num))\n",
    "    print('The top {} terms for topic #{} are: \\n {}'.format(top_n, topic_num, top_terms))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained cv model\n",
    "pickle.dump(cv, open(MODELPATH / 'cv_model', 'wb'))\n",
    "\n",
    "# save trained LDA model\n",
    "pickle.dump(LDA, open(MODELPATH / 'lda_model', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('azureml_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
